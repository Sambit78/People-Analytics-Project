library("NLP")
library("twitteR")
library("syuzhet")
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(plotly)
library(tidyquant)
library(lubridate)
library(rtweet)
# 2. Connect to Twitter ----
api_key = "cfth4IT2X1rfNHjdVzmCdbXR7" # your api_key
api_secret = "GuYx6gaDafnpQuACT3VXPPGUaAUJlCJ5vAdd0JqqqWtyqwwib3" # your api_secret
access_token = "161584380-TdaCfpwat2dErqR0Qn0pVMvadtXPk1mMpzXcErfy" # your access_token
access_token_secret = "3BMV1N2wIQuIkIT4OnxfilirZDAXcIh5PdH0NCxplm6Lb" # your access_token_sceret
#download.file(url = "http://curl.haxx.se/ca/cacert.pem",
#              destfile = "cacert.pem")
setup_twitter_oauth(api_key,
api_secret, # api secret
access_token, # access token
access_token_secret # access token secret
)
# 3. Extract Tweets ----
tweets_biggboss <- searchTwitter("#BB12", lang = "en",since="2018-10-07",until = "2018-10-12",n=17000)
# 1. Library installation ----
#install.packages("twitteR")
#install.packages("ROAuth")
#install.packages("syuzhet")
library("NLP")
library("twitteR")
library("syuzhet")
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(plotly)
library(tidyquant)
library(lubridate)
library(rtweet)
# 2. Connect to Twitter ----
api_key = "cfth4IT2X1rfNHjdVzmCdbXR7" # your api_key
api_secret = "GuYx6gaDafnpQuACT3VXPPGUaAUJlCJ5vAdd0JqqqWtyqwwib3" # your api_secret
access_token = "161584380-TdaCfpwat2dErqR0Qn0pVMvadtXPk1mMpzXcErfy" # your access_token
access_token_secret = "3BMV1N2wIQuIkIT4OnxfilirZDAXcIh5PdH0NCxplm6Lb" # your access_token_sceret
#download.file(url = "http://curl.haxx.se/ca/cacert.pem",
#              destfile = "cacert.pem")
setup_twitter_oauth(api_key,
api_secret, # api secret
access_token, # access token
access_token_secret # access token secret
)
# 3. Extract Tweets ----
tweets_biggboss <- searchTwitter("#BB12", lang = "en",since="2018-10-07",until = "2018-10-12",n=7000)
tweets_biggboss <- searchTwitter("#BB12", lang = "en",since="2018-10-07",until = "2018-10-12",n=7000)
tweets_metoo    <- searchTwitter("#MeToo",lang = "en",since="2018-10-07",until = "2018-10-12",
geocode = "18.975,72.827778,2000mi",n=7000)
# 1. Library installation ----
#install.packages("twitteR")
#install.packages("ROAuth")
#install.packages("syuzhet")
library("NLP")
library("twitteR")
library("syuzhet")
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(plotly)
library(tidyquant)
library(lubridate)
library(rtweet)
# 2. Connect to Twitter ----
api_key = "cfth4IT2X1rfNHjdVzmCdbXR7" # your api_key
api_secret = "GuYx6gaDafnpQuACT3VXPPGUaAUJlCJ5vAdd0JqqqWtyqwwib3" # your api_secret
access_token = "161584380-TdaCfpwat2dErqR0Qn0pVMvadtXPk1mMpzXcErfy" # your access_token
access_token_secret = "3BMV1N2wIQuIkIT4OnxfilirZDAXcIh5PdH0NCxplm6Lb" # your access_token_sceret
#download.file(url = "http://curl.haxx.se/ca/cacert.pem",
#              destfile = "cacert.pem")
setup_twitter_oauth(api_key,
api_secret, # api secret
access_token, # access token
access_token_secret # access token secret
)
# 3. Extract Tweets ----
tweets_biggboss <- searchTwitter("#BB12", lang = "en",since="2018-10-07",until = "2018-10-12",n=7000)
tweets_metoo    <- searchTwitter("#MeToo",lang = "en",since="2018-10-07",until = "2018-10-12",
geocode = "18.975,72.827778,2000mi",n=7000)
# 4. Convert to tidy Dataframe ----
biggboss_tweets <- twListToDF(tweets_biggboss)
metoo_tweets    <- twListToDF(tweets_metoo)
biggboss_tweets1 <- twListToDF(biggboss_rtweet)
# 5. Pre-process and clean the data ----
# Get the text
biggboss_text <- biggboss_tweets$text
metoo_text    <- metoo_tweets$text
#convert all text to lower case
biggboss_text <- tolower(biggboss_text)
metoo_text    <- tolower(metoo_text)
# Replace blank space (“rt”)
biggboss_text <- gsub("rt", "", biggboss_text)
metoo_text    <- gsub("rt", "", metoo_text)
# Replace @UserName
biggboss_text <- gsub("@\\w+", "", biggboss_text)
metoo_text    <- gsub("@\\w+", "", metoo_text)
# Remove punctuation
biggboss_text <- gsub("[[:punct:]]", "", biggboss_text)
metoo_text    <- gsub("[[:punct:]]", "", metoo_text)
# Remove links
biggboss_text <- gsub("http\\w+", "", biggboss_text)
metoo_text    <- gsub("http\\w+", "", metoo_text)
# Remove tabs
biggboss_text  <- gsub("[ |\t]{2,}", "", biggboss_text)
metoo_text     <- gsub("[ |\t]{2,}", "", metoo_text)
# Remove blank spaces at the beginning
biggboss_text <- gsub("^ ", "", biggboss_text)
metoo_text    <- gsub("^ ", "", metoo_text)
# Remove blank spaces at the end
biggboss_text <- gsub(" $", "", biggboss_text)
metoo_text    <- gsub(" $", "", metoo_text)
# 6. Bigg Boss Word Cloud ----
#biggboss_tweets.text.corpus <- tm_map(biggbosscorpus, function(x)removeWords(x,stopwords()))
#metoo_tweets.text.corpus    <- tm_map(metoocorpus, function(x)removeWords(x,stopwords()))
biggbosssample <- sample(biggboss_text,(length(biggboss_text)))
biggbosscorpus <- Corpus(VectorSource(list(biggbosssample)))
biggbosscorpus <- tm_map(biggbosscorpus, removePunctuation)
biggbosscorpus <- tm_map(biggbosscorpus, content_transformer(tolower))
biggbosscorpus <- tm_map(biggbosscorpus, removeNumbers)
biggbosscorpus <- tm_map(biggbosscorpus, stripWhitespace)
biggbosscorpus <- tm_map(biggbosscorpus, removeWords, stopwords('english'))
biggbosscorpus <- tm_map(biggbosscorpus, removeWords, c("biggboss12","biggboss","bb12","not"))
biggbosscorpus <- tm_map(biggbosscorpus, stemDocument)
wordcloud(biggbosscorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2.1,0.23),rot.per=0.2,random.color = TRUE,max.words = 500)
# 7. Me Too  Word Cloud ----
#biggboss_tweets.text.corpus <- tm_map(biggbosscorpus, function(x)removeWords(x,stopwords()))
#metoo_tweets.text.corpus    <- tm_map(metoocorpus, function(x)removeWords(x,stopwords()))
metoosample <- sample(metoo_text,(length(metoo_text)))
metoocorpus <- Corpus(VectorSource(list(metoo_text)))
metoocorpus <- tm_map(metoocorpus, removePunctuation)
metoocorpus <- tm_map(metoocorpus, content_transformer(tolower))
metoocorpus <- tm_map(metoocorpus, removeNumbers)
metoocorpus <- tm_map(metoocorpus, stripWhitespace)
metoocorpus <- tm_map(metoocorpus, removeWords, stopwords('english'))
metoocorpus <- tm_map(metoocorpus, removeWords, c("metoo","bollywood","bb12","not"))
metoocorpus <- tm_map(metoocorpus, stemDocument)
wordcloud(metoocorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2.2,0.56),rot.per=0.3,random.color = TRUE,max.words = 500)
# 8. Sentiment Analysis ----
#getting emotions using in-built function
mysentiment_biggboss <- get_nrc_sentiment((biggboss_text))
mysentiment_metoo    <- get_nrc_sentiment((metoo_text))
#calculationg total score for each sentiment
Sentimentscores_biggboss <-data.frame(colSums(mysentiment_biggboss[,]))
names(Sentimentscores_biggboss) <-"Score"
Sentimentscores_biggboss <-cbind("sentiment"=rownames(Sentimentscores_biggboss),Sentimentscores_biggboss)
rownames(Sentimentscores_biggboss)<-NULL
Sentimentscores_metoo         <-data.frame(colSums(mysentiment_metoo[,]))
names(Sentimentscores_metoo)  <-"Score"
Sentimentscores_metoo         <-cbind("sentiment"=rownames(Sentimentscores_metoo),Sentimentscores_metoo)
rownames(Sentimentscores_metoo)<-NULL
# 9. Sentiment Analysis Plots ----
Sentimentscores_biggboss %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about Bigg Boss 12")
Sentimentscores_metoo %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none") +
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about India's Metoo moment")
# Web Reference
# https://hackernoon.com/text-processing-and-sentiment-analysis-of-twitter-data-22ff5e51e14c
Sentimentscores_biggboss %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about Bigg Boss 12")+
theme(plot.title = element_text(hjust = 0.5))
Sentimentscores_metoo %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none") +
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about India's Metoo moment")+
theme(plot.title = element_text(hjust = 0.5))
# 1. Library installation ----
#install.packages("twitteR")
#install.packages("ROAuth")
#install.packages("syuzhet")
library("NLP")
library("twitteR")
library("syuzhet")
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library(dplyr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(plotly)
library(tidyquant)
library(lubridate)
library(rtweet)
# 2. Connect to Twitter ----
api_key = "cfth4IT2X1rfNHjdVzmCdbXR7" # your api_key
api_secret = "GuYx6gaDafnpQuACT3VXPPGUaAUJlCJ5vAdd0JqqqWtyqwwib3" # your api_secret
access_token = "161584380-TdaCfpwat2dErqR0Qn0pVMvadtXPk1mMpzXcErfy" # your access_token
access_token_secret = "3BMV1N2wIQuIkIT4OnxfilirZDAXcIh5PdH0NCxplm6Lb" # your access_token_sceret
#download.file(url = "http://curl.haxx.se/ca/cacert.pem",
#              destfile = "cacert.pem")
setup_twitter_oauth(api_key,
api_secret, # api secret
access_token, # access token
access_token_secret # access token secret
)
# 3. Extract Tweets ----
tweets_biggboss <- searchTwitter("#BB12", lang = "en",since="2018-10-11",until = "2018-10-12",n=7000)
tweets_metoo    <- searchTwitter("#MeToo",lang = "en",since="2018-10-11",until = "2018-10-12",
geocode = "18.975,72.827778,2000mi",n=7000)
# 4. Convert to tidy Dataframe ----
biggboss_tweets <- twListToDF(tweets_biggboss)
metoo_tweets    <- twListToDF(tweets_metoo)
biggboss_tweets1 <- twListToDF(biggboss_rtweet)
# 5. Pre-process and clean the data ----
# Get the text
biggboss_text <- biggboss_tweets$text
metoo_text    <- metoo_tweets$text
#convert all text to lower case
biggboss_text <- tolower(biggboss_text)
metoo_text    <- tolower(metoo_text)
# Replace blank space (“rt”)
biggboss_text <- gsub("rt", "", biggboss_text)
metoo_text    <- gsub("rt", "", metoo_text)
# Replace @UserName
biggboss_text <- gsub("@\\w+", "", biggboss_text)
metoo_text    <- gsub("@\\w+", "", metoo_text)
# Remove punctuation
biggboss_text <- gsub("[[:punct:]]", "", biggboss_text)
metoo_text    <- gsub("[[:punct:]]", "", metoo_text)
# Remove links
biggboss_text <- gsub("http\\w+", "", biggboss_text)
metoo_text    <- gsub("http\\w+", "", metoo_text)
# Remove tabs
biggboss_text  <- gsub("[ |\t]{2,}", "", biggboss_text)
metoo_text     <- gsub("[ |\t]{2,}", "", metoo_text)
# Remove blank spaces at the beginning
biggboss_text <- gsub("^ ", "", biggboss_text)
metoo_text    <- gsub("^ ", "", metoo_text)
# Remove blank spaces at the end
biggboss_text <- gsub(" $", "", biggboss_text)
metoo_text    <- gsub(" $", "", metoo_text)
# 6. Bigg Boss Word Cloud ----
#biggboss_tweets.text.corpus <- tm_map(biggbosscorpus, function(x)removeWords(x,stopwords()))
#metoo_tweets.text.corpus    <- tm_map(metoocorpus, function(x)removeWords(x,stopwords()))
biggbosssample <- sample(biggboss_text,(length(biggboss_text)))
biggbosscorpus <- Corpus(VectorSource(list(biggbosssample)))
biggbosscorpus <- tm_map(biggbosscorpus, removePunctuation)
biggbosscorpus <- tm_map(biggbosscorpus, content_transformer(tolower))
biggbosscorpus <- tm_map(biggbosscorpus, removeNumbers)
biggbosscorpus <- tm_map(biggbosscorpus, stripWhitespace)
biggbosscorpus <- tm_map(biggbosscorpus, removeWords, stopwords('english'))
biggbosscorpus <- tm_map(biggbosscorpus, removeWords, c("biggboss12","biggboss","bb12","not"))
biggbosscorpus <- tm_map(biggbosscorpus, stemDocument)
wordcloud(biggbosscorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2.1,0.23),rot.per=0.2,random.color = TRUE,max.words = 500)
# 7. Me Too  Word Cloud ----
#biggboss_tweets.text.corpus <- tm_map(biggbosscorpus, function(x)removeWords(x,stopwords()))
#metoo_tweets.text.corpus    <- tm_map(metoocorpus, function(x)removeWords(x,stopwords()))
metoosample <- sample(metoo_text,(length(metoo_text)))
metoocorpus <- Corpus(VectorSource(list(metoo_text)))
metoocorpus <- tm_map(metoocorpus, removePunctuation)
metoocorpus <- tm_map(metoocorpus, content_transformer(tolower))
metoocorpus <- tm_map(metoocorpus, removeNumbers)
metoocorpus <- tm_map(metoocorpus, stripWhitespace)
metoocorpus <- tm_map(metoocorpus, removeWords, stopwords('english'))
metoocorpus <- tm_map(metoocorpus, removeWords, c("metoo","bollywood","bb12","not"))
metoocorpus <- tm_map(metoocorpus, stemDocument)
wordcloud(metoocorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2.2,0.56),rot.per=0.3,random.color = TRUE,max.words = 500)
# 8. Sentiment Analysis ----
#getting emotions using in-built function
mysentiment_biggboss <- get_nrc_sentiment((biggboss_text))
mysentiment_metoo    <- get_nrc_sentiment((metoo_text))
#calculationg total score for each sentiment
Sentimentscores_biggboss <-data.frame(colSums(mysentiment_biggboss[,]))
names(Sentimentscores_biggboss) <-"Score"
Sentimentscores_biggboss <-cbind("sentiment"=rownames(Sentimentscores_biggboss),Sentimentscores_biggboss)
rownames(Sentimentscores_biggboss)<-NULL
Sentimentscores_metoo         <-data.frame(colSums(mysentiment_metoo[,]))
names(Sentimentscores_metoo)  <-"Score"
Sentimentscores_metoo         <-cbind("sentiment"=rownames(Sentimentscores_metoo),Sentimentscores_metoo)
rownames(Sentimentscores_metoo)<-NULL
# 9. Sentiment Analysis Plots ----
Sentimentscores_biggboss %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about Bigg Boss 12")+
theme(plot.title = element_text(hjust = 0.5))
Sentimentscores_metoo %>%
filter(sentiment!=c("negative","positive"))%>%
ggplot(aes(x=sentiment,y=Score))+geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none") +
xlab("Sentiments")+ylab("scores")+ggtitle("Sentiments of people talking about India's Metoo moment")+
theme(plot.title = element_text(hjust = 0.5))
# Web Reference
# https://hackernoon.com/text-processing-and-sentiment-analysis-of-twitter-data-22ff5e51e14c
wordcloud(biggbosscorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2,0.23),rot.per=0.2,random.color = TRUE,max.words = 500)
#
wordcloud(metoocorpus,min.freq = 10,
colors=brewer.pal(8, "Dark2"),scale=c(2.2,0.56),rot.per=0.3,random.color = TRUE,max.words = 500)
# 8.
#Chapter 4Diversity CASE Group level data (1).sav
# https://www.youtube.com/watch?v=G-hOmmtIf90
# Load the libraries
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/Diversity Analysis")
#install.packages("corrplot")
#install.packages("ggpubr")
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(caret)
library(skimr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(UsingR)
library(GGally)
# Load the MASS library
library(MASS)
library(car)
#1.Read SPSS File ----
dataset <- read.spss("ethnicdiversitydata.sav",to.data.frame = TRUE)
summary(dataset)
glimpse(dataset)
####
#2.Determine Predictor Variables ----
# Create a list of interesting predictor variables
####
# How do I restrict my data frame to just these columns that interest me?
# Make a vector of the names;
keep.vars <- c('LondonorNot','Function','GroupSize','NumberFeMaleTeamLeads','PercentMale','BAME');
# Note that the R data frame is a (rectangular) list object which means that it can be
# accessed in two ways - as a matrix or as a list;
# Note that the keep.vars are the COLUMNS that we want to keep, not the rows;
skinny.df <- dataset[,keep.vars];
glimpse(skinny.df)
####
# 3 Omit Missing Values ----
# Delete observations with missing values
####
sample.df <- na.omit(skinny.df);
summary(sample.df)
####
# 4 Train and Test Set ----
####
set.seed(123)
sample.df$u <- runif(n=dim(sample.df)[1],min=0,max=1);
train.df <- subset(sample.df,u<0.70);
test.df  <- subset(sample.df,u>=0.70);
# Check the data
dim(sample.df)[1]
dim(train.df)[1]
dim(test.df)[1]
dim(train.df)[1] + dim(test.df)[1]
# 5 Fit a MLR model ----
model <- lm(BAME ~ ., data=train.df)
# Display model summary
summary(model)
# London and Professional Service Function are key drivers in variation of BAME
# Panel the plots
par(mfrow = c(1, 2), oma = c(0, 0, 2, 0))
plot(model)
par(mfrow = c(1, 1), oma = c(0, 0, 2, 0))
# Use the Base R functon qqplot() to assess the normality of the residuals
qqnorm(model$residuals)
qqline(model$residuals)
# Make a scatterplot
plot(train.df$BAME,model$residuals)
title('Residual vs Predictor')
# Anova Test
P <- aov(BAME ~ .,data = train.df)
summary(P)
# 6 AIC  ----
Measurement <- data.frame(AIC(model))
# 7 MSE and MAE ----
Measurement$MSE <- mean(model$residuals^2)
Measurement$MAE <- mean(abs(model$residuals))
# 7 BIC ----
Measurement$BIC <- BIC(model)
# 8 Adjusted R Square ----
Measurement$Rsquare  <- as.vector(summary(model)$adj.r.squared)
Measurement
# 9 Predictive Accuracy ----
test  <- predict(model,newdata = test.df)
# 10. Mean Square Error (MSE) and Mean Absolute Error (MAE) ----
# calculated as the mean of (Predicted Values - Observed Values)^2?
#The observed values here are the response variable from the testing dataset.
Measurement$MSETest <- mean((test.df$BAME - predict(model, test.df)) ^ 2)
Measurement$MAETest <- mean(abs((test.df$BAME - predict(model, test.df)) ))
Measurement
#Chapter 4Diversity CASE Group level data (1).sav
# https://www.youtube.com/watch?v=G-hOmmtIf90
# Load the libraries
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/Diversity Analysis")
#install.packages("corrplot")
#install.packages("ggpubr")
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(caret)
library(skimr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(UsingR)
library(GGally)
# Load the MASS library
library(MASS)
library(car)
#1.Read SPSS File ----
dataset <- read.spss("ethnicdiversitydata.sav",to.data.frame = TRUE)
summary(dataset)
glimpse(dataset)
dataset <- read.spss("ethnicdiversitydata.sav",to.data.frame = TRUE)
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/People-Analytics-Project/03 - Diversity Analysis - MLR predicting ethnic diversity")
#Chapter 4Diversity CASE Group level data (1).sav
# https://www.youtube.com/watch?v=G-hOmmtIf90
# Load the libraries
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/People-Analytics-Project/03 - Diversity Analysis - MLR predicting ethnic diversity")
#install.packages("corrplot")
#install.packages("ggpubr")
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(caret)
library(skimr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(UsingR)
library(GGally)
# Load the MASS library
library(MASS)
library(car)
#1.Read SPSS File ----
dataset <- read.spss("ethnicdiversitydata.sav",to.data.frame = TRUE)
summary(dataset)
glimpse(dataset)
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/People-Analytics-Project/03 - Diversity Analysis - MLR predicting ethnic diversity")
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(caret)
library(skimr)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(UsingR)
library(GGally)
# Load the MASS library
library(MASS)
library(car)
#1.Read SPSS File ----
dataset <- read.spss("ethnicdiversitydata.sav",to.data.frame = TRUE)
summary(dataset)
glimpse(dataset)
install.packages("foreign")
install.packages("foreign")
